{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a97a2c8e",
   "metadata": {},
   "source": [
    "# ***Automatic Differentiation (Autograd)***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1117f79",
   "metadata": {},
   "source": [
    "- In deep learning, the core idea is to optimize a loss function by adjusting weights in a neural network. This is done through gradient descent, which relies on gradients‚Äîpartial derivatives of the loss function with respect to model parameters.\n",
    "\n",
    "Traditionally, computing these gradients required:\n",
    "- Manual differentiation ‚Äî error-prone and tedious\n",
    "- Symbolic differentiation (like in SymPy) ‚Äî can be inefficient for large networks\n",
    "- Numerical differentiation (finite differences) ‚Äî inaccurate and slow\n",
    "\n",
    "This led to the need for Automatic Differentiation (AD), a technique to compute exact gradients efficiently and automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2c8b26",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è What is Automatic Differentiation?\n",
    "\n",
    "Automatic Differentiation (AD) breaks down computations into a graph of operations and then systematically applies the chain rule to compute gradients.\n",
    "\n",
    "- PyTorch uses a dynamic computation graph, built on the fly during forward pass.\n",
    "- This makes it flexible and intuitive, especially for models with varying control flow (if/else, loops, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffacef5",
   "metadata": {},
   "source": [
    "### üöÄ Enter Autograd: PyTorch‚Äôs Engine for AD\n",
    "\n",
    "autograd is the automatic differentiation engine in PyTorch.\n",
    "\n",
    "- Key ideas:\n",
    "\n",
    "Every Tensor in PyTorch has a ***flag: requires_grad***\n",
    "- When requires_grad=True, PyTorch tracks all operations on that tensor to build the computation graph\n",
    "- Calling .backward() on a scalar loss automatically computes gradients of all tensors involved\n",
    "\n",
    "## üîÑ The Computation Graph\n",
    "- Nodes: tensors\n",
    "- Edges: functions (operations) that produce the next tensors\n",
    "- Leaf Nodes: input tensors\n",
    "- The graph is dynamic: it is re-built after every forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0cc985",
   "metadata": {},
   "source": [
    ">> ***This design is why PyTorch is called \"define-by-run\" (as opposed to \"define-and-run\", like TensorFlow 1.***x)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daabafbd",
   "metadata": {},
   "source": [
    "üß† Behind .backward()\n",
    "- PyTorch uses reverse-mode automatic differentiation, which is efficient when you have:\n",
    "- Many inputs (model parameters)\n",
    "- Single output (loss)\n",
    "\n",
    ".backward() starts at the scalar loss and moves backward through the computation graph applying the chain rule\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254950cb",
   "metadata": {},
   "source": [
    "| Function               | Description                                              |\n",
    "| ---------------------- | -------------------------------------------------------- |\n",
    "| `.backward()`          | Computes gradients                                       |\n",
    "| `.grad`                | Stores the gradient of a tensor                          |\n",
    "| `.zero_grad()`         | Clears old gradients before a new `.backward()`          |\n",
    "| `with torch.no_grad()` | Disables autograd for inference or evaluation            |\n",
    "| `detach()`             | Returns a new tensor detached from the computation graph |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ac8bc5",
   "metadata": {},
   "source": [
    "- Suppose you compute the loss L = (x * w + b)^2, where x, w, and b are tensors.\n",
    "\n",
    "1. During the forward pass, PyTorch builds the graph:\n",
    "\n",
    "multiplies x and w, adds b, squares the result\n",
    "\n",
    "\n",
    "2. You call loss.backward()\n",
    "\n",
    "PyTorch traces back the graph and computes gradients:\n",
    " dl/dw , dl/db\n",
    "\n",
    "3. These are stored in w.grad, b.grad\n",
    "\n",
    "\n",
    "### ‚ö†Ô∏è Things to Remember\n",
    "- You need to zero out gradients after every optimization step (optimizer.zero_grad()), or gradients will accumulate.\n",
    "- Autograd only works with float-type tensors with requires_grad=True.\n",
    "- Operations not involving tensors with requires_grad=True are not tracked.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e242c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = 2.0\n"
     ]
    }
   ],
   "source": [
    " # Create a tensor that requires gradients\n",
    "import torch\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "print(f\"x = {x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee6d1d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = x¬≤ + 3x + 1 = 11.0\n"
     ]
    }
   ],
   "source": [
    " # Define a function\n",
    "y = x**2 + 3*x + 1\n",
    "print(f\"y = x¬≤ + 3x + 1 = {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5eab2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Calculate the gradient (derivative)\n",
    "y.backward()  # This computes dy/dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ff7ae8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient dy/dx = 7.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gradient dy/dx = {x.grad}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17f5e05",
   "metadata": {},
   "source": [
    "## ***More complex example***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d343b33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z = x¬≤ + y¬≥ + xy = 11.0\n",
      "‚àÇz/‚àÇx = 4.0\n",
      "‚àÇz/‚àÇy = 13.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0, requires_grad= True)\n",
    "y = torch.tensor(2.0, requires_grad= True)\n",
    "\n",
    "z = x**2 + y**3 + x*y\n",
    "\n",
    "print(f\"z = x¬≤ + y¬≥ + xy = {z}\")\n",
    "\n",
    "z.backward()\n",
    "\n",
    "print(f\"‚àÇz/‚àÇx = {x.grad}\")\n",
    "print (f\"‚àÇz/‚àÇy = {y.grad}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fd3028",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91e38b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dloss/dx: tensor(60.)\n",
      "dloss/dw: tensor(40.)\n",
      "dloss/db: tensor(20.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "w = torch.tensor(3.0, requires_grad=True)\n",
    "b = torch.tensor(4.0, requires_grad=True)\n",
    "\n",
    "y = w * x + b     # y = 3*2 + 4 = 10\n",
    "loss = y ** 2     # loss = 10^2 = 100\n",
    "\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(\"dloss/dx:\", x.grad)  # Should be 2 * y * w = 2*10*3 = 60\n",
    "print(\"dloss/dw:\", w.grad)  # Should be 2 * y * x = 2*10*2 = 40\n",
    "print(\"dloss/db:\", b.grad)  # Should be 2 * y * 1 = 2*10 = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15f29a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y2 = x**3 - 2*x**2 + x-5\n",
    "\n",
    "y2.backward()\n",
    "\n",
    "print (x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcee1db0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
