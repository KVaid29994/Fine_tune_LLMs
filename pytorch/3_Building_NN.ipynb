{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e8a5f43",
   "metadata": {},
   "source": [
    "## ***üß† Neural Networks***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe512e3b",
   "metadata": {},
   "source": [
    "A Neural Network (NN) is a computational model inspired by the human brain. It consists of layers of neurons (or nodes) that transform input data into meaningful outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94461bf1",
   "metadata": {},
   "source": [
    "### üì¶ Basic Building Blocks\n",
    "\n",
    "1. Input Layer\n",
    "- Accepts the raw input features (e.g., pixels, words, sensor readings)\n",
    "\n",
    "2. Hidden Layers\n",
    "- Perform transformations using learned weights and biases\n",
    "- Apply non-linear activation functions (like ReLU, sigmoid, tanh)\n",
    "\n",
    "3. Output Layer\n",
    "- Produces the final prediction (e.g., class probabilities or regression value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c294a5ac",
   "metadata": {},
   "source": [
    "### üîÑ Forward and Backward Pass\n",
    "‚û°Ô∏è Forward Pass\n",
    "Data flows layer by layer to compute predictions.\n",
    "\n",
    "‚¨ÖÔ∏è Backward Pass (Backpropagation)\n",
    "Uses chain rule to compute gradients of loss w.r.t. weights\n",
    "\n",
    "Gradients are used to update weights (via gradient descent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddda47b",
   "metadata": {},
   "source": [
    "### üìâ Loss Functions\n",
    "\n",
    "- Loss measures how far predictions are from the truth.\n",
    "Examples:\n",
    "- MSE (Mean Squared Error) ‚Äì regression\n",
    "- Cross-Entropy ‚Äì classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a365d844",
   "metadata": {},
   "source": [
    "### üß™ Activation Functions\n",
    "\n",
    "| Function | Use Case                   | Formula                                        |\n",
    "| -------- | -------------------------- | ---------------------------------------------- |\n",
    "| ReLU     | Most common, fast          | $\\text{ReLU}(x) = \\max(0, x)$                  |\n",
    "| Sigmoid  | Binary classification      | $\\sigma(x) = \\frac{1}{1 + e^{-x}}$             |\n",
    "| Tanh     | Zero-centered              | $\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$ |\n",
    "| Softmax  | Multi-class classification | Converts logits to probabilities               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f3dc94",
   "metadata": {},
   "source": [
    "### üß∞ Key PyTorch Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eecf5a",
   "metadata": {},
   "source": [
    "| Concept       | PyTorch Term                  |\n",
    "| ------------- | ----------------------------- |\n",
    "| Model         | `torch.nn.Module`             |\n",
    "| Layer         | `torch.nn.Linear`             |\n",
    "| Activation    | `torch.nn.ReLU`, `F.relu`     |\n",
    "| Loss          | `torch.nn.CrossEntropyLoss()` |\n",
    "| Optimizer     | `torch.optim.Adam()`          |\n",
    "| Training Loop | `.backward()`, `.step()`      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869d39f7",
   "metadata": {},
   "source": [
    "### ‚úÖ Best Practices / Key Pointers\n",
    "- üîÑ Shuffle data before training (use DataLoader)\n",
    "- üß† Normalize inputs for stable training\n",
    "- üßπ Zero out gradients each iteration: optimizer.zero_grad()\n",
    "- üß™ Use validation set to check overfitting\n",
    "- üî• Avoid exploding/vanishing gradients: use ReLU, batch norm\n",
    "- ‚è≥ Save checkpoints and track loss using logging tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b6b03c",
   "metadata": {},
   "source": [
    "## First Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e27452e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
