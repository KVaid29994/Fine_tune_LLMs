{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce45a24a",
   "metadata": {},
   "source": [
    "# ***Fine Tunning LLM models***\n",
    "\n",
    "## ***Quantization***\n",
    "\n",
    "Quantization is the process of converting model weights and activations from ***higher precision formats (e.g., FP32) to lower precision formats*** (e.g., INT8), which reduces memory footprint and boosts inference speed—particularly critical for deploying large models on edge devices or limited-resource systems.\n",
    "\n",
    "- FP32 to FP16 ( conversion is called half precision)\n",
    "\n",
    "### 🚀 ***Benefits of Quantization***\n",
    "- Ability to perfrom fine tunning\n",
    "- Reduced memory usage (e.g., 4x smaller from FP32 → INT8)\n",
    "- Faster inference, especially on CPUs\n",
    "- Lower power consumption\n",
    "- Helps deploy models on mobile, embedded, or edge devices\n",
    "\n",
    "### ❌***Disadvantages of Quantization***\n",
    "- Potential loss of precision (quantization error)\n",
    "- May require calibration or retraining to recover accuracy\n",
    "- Not all operations or hardware support lower precision (e.g., INT8)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## ***Mathematical Intiution***\n",
    "\n",
    "Quantization maps floating-point values to integers. There are two primary types:\n",
    "\n",
    "- 🔷 Symmetric Quantization- ( batch normalisation is a tech of symmetric quantization )\n",
    "    - In symmetric quantization, the zero-point is always zero, and values are assumed to be centered around it.\n",
    "    - Example:\n",
    "        >>- Converting a range [0.0, 1000.0] (FP32) to [0, 255] (UINT8)\n",
    "        - This assumes that the minimum and maximum values are symmetric around zero (or can be shifted accordingly).\n",
    "        - Used when the input distribution is approximately symmetric or zero-centered.\n",
    "\n",
    "    - How does the conversion happen?\n",
    "        - Original floating-point range: Xmin = 0.0, Xmax = 1000.0\n",
    "        - Quantized range: Qmin = 0, Qmax = 255\n",
    "\n",
    "    - Scale factor:\n",
    "        - Single precision fp32  - mantasa and exponent\n",
    "\n",
    "    - Equation - MinMaxScaler (fp1000 to uint8)\n",
    "        - 0.0 to a quantized value of 0\n",
    "        - 1000 to be convered to a value of 255\n",
    "        - the scale is - Xmax-Xmin/Qmax-Qmin\n",
    "        eq - 1000-0/255-0 = 3.92\n",
    "        - scale factor is 3.92, now say between -0 and 1000, if it is 789. then divide by 789/3.92\n",
    "        - use round factor as well\n",
    "\n",
    "---  \n",
    "\n",
    "### 🔶 ***Asymmetric Quantization***\n",
    "\n",
    "    - In asymmetric quantization, the zero-point is non-zero, which allows for better handling of values that are not centered around zero (i.e., skewed distributions).\n",
    "\n",
    "    Example:\n",
    "    - Converting [-20, 1000] (FP32) to [0, 255] (UINT8)\n",
    "\n",
    "    - 1000+20/255 = 4.0\n",
    "    - now if i quantize -20 to -5 with the scale factor of 20\n",
    "    - but i have a negaitve number outisd, so now -5+5 becomes zerop pont.\n",
    "\n",
    "### ***Extra Notes***\n",
    "\n",
    "Batch Normalization layers typically use symmetric quantization due to their balanced input distribution.\n",
    "Choosing between symmetric and asymmetric quantization often depends on:\n",
    "1. The data distribution\n",
    "2. Target hardware compatibility\n",
    "3. Model architecture and tolerance to quantization noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a17304",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99431c35",
   "metadata": {},
   "source": [
    "## POST Training QUANTIZATION (PTQ)\n",
    "\n",
    "- PTQ is applied after a model has been fully trained using high-precision (typically 32-bit float) weights.\n",
    "- The model is calibrated using a small representative dataset to estimate the dynamic ranges (min/max values) of activations and weights.\n",
    "- Quantization maps high-precision parameters (e.g., float32) to lower-precision formats (e.g., int8, uint8) by applying scaling and zero-point shifting.\n",
    "- The quantized model is smaller and runs faster on resource-constrained devices (like edge/IoT/embedded).\n",
    "- PTQ is non-invasive – it doesn’t require retraining the model.\n",
    "\n",
    "However, this technique may lead to significant accuracy degradation, especially for models sensitive to numerical precision (e.g., NLP transformers or object detection models).\n",
    "\n",
    "> ✅ Best for: Models already trained and deployed where retraining is expensive or impossible. Suitable for simpler or more robust models like MobileNet, ResNet on classification tasks\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Quantization-Aware Training (QAT)\n",
    "\n",
    "***QAT simulates quantization effects during training to make the model robust to quantization noise.***\n",
    "\n",
    "Process:\n",
    "- Start with a trained float32 model.\n",
    "- Inject fake quantization operations (called \"fake quant ops\") during forward/backward passes to mimic int8 behavior.\n",
    "- Fine-tune the model with training data to adapt the weights for lower precision.\n",
    "- Export the final model to a fully quantized version.\n",
    "\n",
    "- QAT results in higher accuracy retention compared to PTQ, especially for complex models or tasks involving fine-grained numerical precision.\n",
    "\n",
    "- Supports per-channel quantization, activation clipping, and more advanced techniques that further minimize accuracy loss.\n",
    "\n",
    ">> ✅ Best for: When accuracy is critical or PTQ causes unacceptable performance drops. Commonly used in production ML systems (e.g., TensorFlow Lite QAT, PyTorch FX QAT, ONNX QAT workflows).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6b8a5a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
